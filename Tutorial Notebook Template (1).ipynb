{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8bd3a486-36c6-4c20-94d7-a30271ad011d",
      "metadata": {},
      "source": [
        "# **Foundational NLP**\n",
        "\n",
        "## **List comprehensions are fast, but generators are faster!?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73caf684",
      "metadata": {},
      "source": [
        "# **Table of Contents**\n",
        "\n",
        "1.   [Introduction](#Introduction)\n",
        "2.   [Prerequisites](#Prerequisites)\n",
        "3.   [Step-by-Step-Guide](#Step-by-Step-Guide)\n",
        "4.   [Code Examples](#Code-Examples)\n",
        "5.   [Troubleshooting](#Troubleshooting)\n",
        "6.   [Conclusion](#Conclusion)\n",
        "7.   [References](#References)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b8a541-a2e4-4886-b42e-4126d9fb9d7a",
      "metadata": {},
      "source": [
        "## **Introduction**\n",
        "The principle of distributional semantics is encapsulated in J.R. Firth’s famous quote   <pre> ```“You shall know a word by the company it keeps”``` </pre> \n",
        " this quote highlights the significance of contextual information in determining   \n",
        " word meaning and captures the importance of contextual information in defining word meanings.   \n",
        " This principle is a cornerstone in the development of word embeddings.\n",
        "\n",
        "Word embeddings, also known as word vectors, provide a dense, continuous, and compact representation of words,  \n",
        "encapsulating their semantic and syntactic attributes.   \n",
        "They are essentially real-valued vectors, and the proximity of these vectors in a multidimensional   \n",
        "space is indicative of the linguistic relationships between words \n",
        "\n",
        "The term  <pre> “embedding” </pre>  in this context refers to the transformation of discrete words into   \n",
        "continuous vectors,   \n",
        "achieved through word embedding algorithms. These algorithms are designed to convert   \n",
        "words into vectors that encapsulate a significant portion of their semantic content.   \n",
        "An example of the effectiveness of these embeddings is the vector arithmetic that yields meaningful analogies such as <pre> \"uncle\" - \"man\" + \"woman\" ≈ \"aunt\" </pre>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b84706-8034-464e-aeb9-3a40249370df",
      "metadata": {},
      "source": [
        "## **Prerequisites**\n",
        "\n",
        "- Programming fundamentals (Python is the standard language for NLP)\n",
        "\n",
        "- Basic probability and statistics as well as linear algebra concepts\n",
        "\n",
        "- Machine learning concepts\n",
        "\n",
        "- Text preprocessing techniques\n",
        "\n",
        "- Linguistic Terminology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a37af23-9129-4e73-88a0-b8714f639416",
      "metadata": {},
      "source": [
        "<a id='guide'></a>\n",
        "## **Step-by-Step Guide**\n",
        "\n",
        "## Word Embedding Techniques\n",
        "\n",
        "- Count-Based Techniques (TF-IDF and BM25)  \n",
        "- Co-occurrence Based/Static Embedding Techniques  \n",
        "- Contextualized/Dynamic Representation Techniques (BERT, ELMo) \n",
        "\n",
        "\n",
        "### Bag of Words (BoW) \n",
        "\n",
        "Tokenization:\n",
        " - Split the text into words (tokens).  \n",
        "\n",
        "Vocabulary Building:\n",
        " - Create a vocabulary list of all unique words in the corpus.\n",
        "\n",
        "Vector Representation:\n",
        "   - For each document, create a vector where each element corresponds to a word in the vocabulary. \n",
        "     The value is the count of occurrences of that word in the document.\n",
        "\n",
        "\n",
        "\n",
        "**Example** \n",
        "\n",
        "Consider a corpus with the following two documents:\n",
        "1. “The cat sat on the mat.”\n",
        "2. “The dog sat on the log.”\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Tokenization:\n",
        "   - Document 1: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "   - Document 2: [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"]\n",
        "\n",
        "\n",
        "2. Vocabulary Building:\n",
        "    - Vocabulary: [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"log\"]\n",
        "\n",
        "\n",
        "3. Vector Representation:\n",
        "   - Document 1: [2, 1, 1, 1, 1, 0, 0]\n",
        "   - Document 2: [2, 0, 1, 1, 0, 1, 1]\n",
        "\n",
        "    The resulting BoW vectors are:\n",
        "   - Document 1: [2, 1, 1, 1, 1, 0, 0]\n",
        "   - Document 2: [2, 0, 1, 1, 0, 1, 1]\n",
        "\n",
        "\n",
        "\n",
        "###  Term Frequency-Inverse Document Frequency (TF-IDF)  \n",
        "\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a \n",
        "word to a document in a collection or corpus. \n",
        "It is a fundamental technique in text processing that ranks the \n",
        "relevance of documents to a specific query, commonly applied in tasks such as document classification, search engine ranking, \n",
        "information retrieval, and text mining."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b82436ec-e867-4841-966b-9c3fd6280850",
      "metadata": {},
      "source": [
        "## **Code Examples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "05df820e-c75f-4f72-9be3-8040a9eb9c9c",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "\n",
        "def plainlist(n=100000):\n",
        "    my_list = []\n",
        "    for i in range(n):\n",
        "        if i % 5 == 0:\n",
        "            my_list.append(i)\n",
        "    return my_list\n",
        "\n",
        "\n",
        "def listcompr(n=100000):\n",
        "    my_list = [i for i in range(n) if i % 5 == 0]\n",
        "    return my_list\n",
        "\n",
        "\n",
        "def generator(n=100000):\n",
        "    my_gen = (i for i in range(n) if i % 5 == 0)\n",
        "    return my_gen\n",
        "\n",
        "\n",
        "def generator_yield(n=100000):\n",
        "    for i in range(n):\n",
        "        if i % 5 == 0:\n",
        "            yield i"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc822ff-e260-4ddc-91c3-4347d6a7fff6",
      "metadata": {},
      "source": [
        "**To be fair to the list, let us exhaust the generators:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "81c397af-393a-4339-a265-5d86b8db73b8",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plain_list:     3.81 ms ± 57.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "\n",
            "listcompr:     3.68 ms ± 66.8 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "\n",
            "generator:     3.79 ms ± 176 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "\n",
            "generator_yield:     3.65 ms ± 74.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "def test_plainlist(plain_list):\n",
        "    for i in plain_list():\n",
        "        pass\n",
        "\n",
        "\n",
        "def test_listcompr(listcompr):\n",
        "    for i in listcompr():\n",
        "        pass\n",
        "\n",
        "\n",
        "def test_generator(generator):\n",
        "    for i in generator():\n",
        "        pass\n",
        "\n",
        "\n",
        "def test_generator_yield(generator_yield):\n",
        "    for i in generator_yield():\n",
        "        pass\n",
        "\n",
        "\n",
        "print('plain_list:     ', end='')\n",
        "%timeit test_plainlist(plainlist)\n",
        "print('\\nlistcompr:     ', end='')\n",
        "%timeit test_listcompr(listcompr)\n",
        "print('\\ngenerator:     ', end='')\n",
        "%timeit test_generator(generator)\n",
        "print('\\ngenerator_yield:     ', end='')\n",
        "%timeit test_generator_yield(generator_yield)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6207c961-9400-43bf-8b3d-68b1d386d0b5",
      "metadata": {},
      "source": [
        "## **Troubleshooting**\n",
        "\n",
        "Common pitfalls when working with generators and list comprehensions, such as:\n",
        "\n",
        "Why generators don’t support indexing\n",
        "\n",
        "*   How to convert a generator to a list when needed\n",
        "*   Debugging performance issues in large data processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21e5569-95d9-4a75-8c22-751b3c05a02f",
      "metadata": {},
      "source": [
        "## **Conclusion**\n",
        "\n",
        "A summary of key takeaways, reinforcing that while list comprehensions are faster for small datasets, generators excel in memory efficiency for large data streams. A recommendation on when to use each method will be provided."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b9704f8-cc01-4159-841f-de6d7973e92b",
      "metadata": {},
      "source": [
        "## **References**\n",
        "\n",
        "Links to Python documentation, performance benchmarking resources, and other relevant articles for further reading."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7c0266-7f5b-4247-ac36-c6b44f90bf03",
      "metadata": {},
      "source": [
        "# **Facilitator(s) Details**\n",
        "\n",
        "**Facilitator(s):**\n",
        "\n",
        "*   Name: FELIX TETTEH AKWERH\n",
        "*   Email: felix.akwerh@knust.edu.gh\n",
        "*   LinkedIn: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7257b3a4-6572-417d-8b22-b53efb4bd6b2",
      "metadata": {},
      "source": [
        "# **Reviewer’s Name**\n",
        "\n",
        "*   Name: [Reviewer’s Name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c22f7673-72d0-4bf2-bc87-f9c7c2c4b32c",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
